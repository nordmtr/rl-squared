{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "\n",
        "# RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning\n",
        "\n",
        "**OpenAI (2016)**\n",
        "\n",
        "- Meta-learning across tasks, fast adaptation within a task\n",
        "- Learner lives in hidden state\n",
        "- Clean prototype of a \"foundation model over tasks\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "\n",
        "# RL primer + why RL^2\n",
        "\n",
        "- RL: choose actions to maximize expected return\n",
        "- Standard RL is slow (parameter updates)\n",
        "- RL^2: fast learning inside the policy, slow RL trains it\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "\n",
        "# The money slide: learner in activations\n",
        "\n",
        "```\n",
        "(obs_t, action_{t-1}, reward_{t-1}, done_{t-1})\n",
        "                     |\n",
        "                     v\n",
        "               RNN policy\n",
        "                     |\n",
        "                     v\n",
        "                  action_t\n",
        "\n",
        "hidden state h_t carries across episodes\n",
        "```\n",
        "\n",
        "- Same weights, different behavior as h_t changes\n",
        "- \"Learning\" = updating h, not theta\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b522ae48",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "\n",
        "# Training setup: slow RL over tasks\n",
        "\n",
        "- Sample a task each trial\n",
        "- Multiple episodes so state can adapt\n",
        "- Maximize expected return across tasks\n",
        "\n",
        "Foundation model analogy: pretraining = prior; context/state = adaptation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c5b8a78",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "\n",
        "# Connections + limitations + what next\n",
        "\n",
        "**Connections**\n",
        "- In-context learning: slow weights, fast context/state\n",
        "- TabPFN / TabICL: ICL-style tabular models\n",
        "\n",
        "**Limitations**\n",
        "- Distribution dependence\n",
        "- Exploration + credit assignment are fragile\n",
        "\n",
        "**Papers that poke**\n",
        "- VSML (representational bottleneck)\n",
        "- Learned optimizers / update rules\n",
        "\n",
        "Closing thought: multiple time scales (slow priors + fast adaptation)\n"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
